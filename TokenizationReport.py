import json
from datetime import datetime
import constants

def generate_tokenization_report(summary_path=None, output_path=None):
    summary_path = summary_path or constants.get_log_path("validate_corpus.summary.json")
    output_path  = output_path or constants.get_log_path( constants.DS_TOKENIZATION_REPORT )

    try:
        with open(summary_path, encoding="utf-8") as f:
            summary = json.load(f)
    except Exception as e:
        print(f"🚨 Failed to load summary file: {e}")
        return

    # Extract values
    total     = summary.get("total_samples", 0)
    avg_len   = summary.get("avg_input_length", 0)
    mismatch  = summary.get("label_mismatches", 0)
    sarcasm   = summary.get("sarcasm_token_distribution", {})
    tone_lens = summary.get("tone_sequence_lengths", {})

    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Build interpretation lines
    interp_lines = []

    # Avg input length
    if avg_len < 128:
        interp_lines.append(f"🔴 Average input length is very low (**{avg_len} tokens**), which may underutilize the model’s capacity.")
    elif avg_len > 512:
        interp_lines.append(f"🟠 Average input length exceeds common limits (**{avg_len} tokens**); risk of truncation.")
    else:
        interp_lines.append(f"🟢 Average input length of **{avg_len} tokens** is within optimal range.")

    # Label mismatches
    if mismatch > 0:
        interp_lines.append(f"🔴 Found **{mismatch}** label mismatches — tokenization or mask alignment may be broken.")
    else:
        interp_lines.append(f"🟢 No label mismatches detected — masking appears clean.")

    # Sarcasm distribution
    if sarcasm:
        mode = max(sarcasm, key=sarcasm.get)
        interp_lines.append(f"🟢 Sarcasm levels are varied; most frequent token is `{mode}` with `{sarcasm[mode]}` samples.")
    else:
        interp_lines.append("🟠 Sarcasm token distribution missing — tone targets may be absent.")

    # Tone sequence lengths
    if 0 in tone_lens:
        interp_lines.append(f"🟠 Found samples with **zero tone tags** — consider filtering those before training.")
    else:
        most_common_len = max(tone_lens, key=tone_lens.get)
        interp_lines.append(f"🟢 Majority of tone sequences contain `{most_common_len}` tags — aligns with typical annotations.")

    # Build Markdown
    md = f"""# 🧪 Tokenized Corpus Validation Report

Generated: **{now}**  
Corpus Path: `{constants.DIR_TRAINING_CORPUS}`  

---

## 🔍 Corpus Statistics

| Metric | Value |
|--------|-------|
| Total Samples | `{total}` |
| Average Input Length | `{avg_len}` tokens |
| Label Mismatches | `{mismatch}` |
| Sarcasm Token Distribution | `{sarcasm}` |
| Tone Sequence Lengths | `{tone_lens}` |

---

## 🧠 Interpretation

""" + "\n".join(f"- {line}" for line in interp_lines) + """

---

## ✅ Verdict

The corpus has been validated.  
Review any orange or red indicators above before training.  

---

## 📁 Artifacts

- Summary: `validate_corpus.summary.json`  
- Log: `validate_corpus.log`

---

*Report generated by Copilot 🧪*
"""

    try:
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(md)
        print(f"✅ Markdown report written to: {output_path}")
    except Exception as e:
        print(f"🚨 Failed to write report: {e}")